{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 2, Sprint 1, Module 3*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "- Do one-hot encoding of categorical features\n",
    "- Do univariate feature selection\n",
    "- Use scikit-learn to fit Ridge Regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab (run the code cell below).\n",
    "\n",
    "Libraries:\n",
    "- category_encoders\n",
    "- matplotlib\n",
    "- numpy\n",
    "- pandas\n",
    "- scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "# If you're on Colab:\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Applied-Modeling/master/data/'\n",
    "    !pip install category_encoders==2.*\n",
    "\n",
    "# If you're working locally:\n",
    "else:\n",
    "    DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do one-hot encoding of categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the NYC apartment rental listing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read New York City apartment rental listing data\n",
    "df = pd.read_csv(DATA_PATH+'apartments/renthop-nyc.csv')\n",
    "assert df.shape == (49352, 34)\n",
    "\n",
    "# Remove the most extreme 1% prices,\n",
    "# the most extreme .1% latitudes, &\n",
    "# the most extreme .1% longitudes\n",
    "df = df[(df['price'] >= np.percentile(df['price'], 0.5)) & \n",
    "        (df['price'] <= np.percentile(df['price'], 99.5)) & \n",
    "        (df['latitude'] >= np.percentile(df['latitude'], 0.05)) & \n",
    "        (df['latitude'] < np.percentile(df['latitude'], 99.95)) &\n",
    "        (df['longitude'] >= np.percentile(df['longitude'], 0.05)) & \n",
    "        (df['longitude'] <= np.percentile(df['longitude'], 99.95))]\n",
    "\n",
    "# Do train/test split\n",
    "# Use data from April & May 2016 to train\n",
    "# Use data from June 2016 to test\n",
    "df['created'] = pd.to_datetime(df['created'], infer_datetime_format=True)\n",
    "cutoff = pd.to_datetime('2016-06-01')\n",
    "train = df[df.created < cutoff]\n",
    "test  = df[df.created >= cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some columns are numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some columns are _not_ numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the relationship between `interest_level` and `price`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interest Level seems like a useful, predictive feature. But it's a string — and our scikit-learn models expect all inputs to be numbers. \n",
    "\n",
    "So, we can \"one-hot encode\" the feature.\n",
    "\n",
    "To go from this:\n",
    "\n",
    "| interest_level |\n",
    "|----------------|\n",
    "| high           |\n",
    "| medium         |\n",
    "| low            |\n",
    "\n",
    "\n",
    "To this:\n",
    "\n",
    "| interest_level_high | interest_level_medium | interest_level_low |\n",
    "|---------------------|-----------------------|--------------------|\n",
    "| 1                   | 0                     | 0                  |\n",
    "| 0                   | 1                     | 0                  |\n",
    "| 0                   | 0                     | 1                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"One-hot encoding\" adds a dimension for each unique value of each categorical feature. So, it may not be a good choice for \"high cardinality\" categoricals that have dozens, hundreds, or thousands of unique values. \n",
    "\n",
    "[Cardinality](https://simple.wikipedia.org/wiki/Cardinality) means the number of unique values that a feature has:\n",
    "> In mathematics, the cardinality of a set means the number of its elements. For example, the set A = {2, 4, 6} contains 3 elements, and therefore A has a cardinality of 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other non-numeric columns have high cardinality. So let's exclude them from our features for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what `X_train` looks like **before** encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [OneHotEncoder](https://contrib.scikit-learn.org/category_encoders/onehot.html) from the [category_encoders](https://github.com/scikit-learn-contrib/category_encoders) library to encode any non-numeric features. (In this case, it's just `interest_level`.)\n",
    "\n",
    "- Use the **`fit_transform`** method on the **train** set\n",
    "- Use the **`transform`** method on **validation / test** sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what it looks like **after:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your assignment, you will do one-hot encoding of categorical features with feasible cardinality, using the category_encoders library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do univariate feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous assignment quoted Wikipedia on [Feature Engineering](https://en.wikipedia.org/wiki/Feature_engineering):\n",
    "\n",
    "> \"Some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.\" — Pedro Domingos, [\"A Few Useful Things to Know about Machine Learning\"](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "\n",
    "> \"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied machine learning' is basically feature engineering.\" — Andrew Ng, [Machine Learning and AI via Brain simulations](https://forum.stanford.edu/events/2011/2011slides/plenary/2011plenaryNg.pdf) \n",
    "\n",
    "> Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. \n",
    "\n",
    "Pedro Domingos says, \"the most important factor is the **features used**.\"\n",
    "\n",
    "This includes not just **Feature Engineering** (making new features, representing features in new ways) but also **Feature Selection** (choosing which features to include and which to exclude).\n",
    "\n",
    "There are _many_ specific tools and techniques for feature selection.\n",
    "\n",
    "- Today we'll try [scikit-learn's `SelectKBest` transformer](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection), for \"univariate, forward selection.\"\n",
    "- Later we'll try another technique, [\"permutation importance\"](https://www.kaggle.com/dansbecker/permutation-importance)\n",
    "- If you want to explore even more options, here are some good resources!\n",
    "  - [scikit-learn's User Guide for Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "  - [mlxtend](http://rasbt.github.io/mlxtend/) library\n",
    "  - scikit-learn-contrib libraries: [boruta_py](https://github.com/scikit-learn-contrib/boruta_py) & [stability-selection](https://github.com/scikit-learn-contrib/stability-selection)\n",
    "  - [_Feature Engineering and Selection_](http://www.feat.engineering/) by Kuhn & Johnson.\n",
    "\n",
    "\n",
    "My general recommendation is:\n",
    "\n",
    "> Predictive accuracy on test sets is the criterion for how good the model is. — Leo Breiman, [\"Statistical Modeling: The Two Cultures\"](https://projecteuclid.org/download/pdf_1/euclid.ss/1009213726)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's engineer a few more features to select from. This is a partial example solution from the previous assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def engineer_features(X):\n",
    "    \n",
    "    # Avoid SettingWithCopyWarning\n",
    "    X = X.copy()\n",
    "    \n",
    "    # How many total perks does each apartment have?\n",
    "    perk_cols = ['elevator', 'cats_allowed', 'hardwood_floors', 'dogs_allowed',\n",
    "                 'doorman', 'dishwasher', 'no_fee', 'laundry_in_building',\n",
    "                 'fitness_center', 'pre-war', 'laundry_in_unit', 'roof_deck',\n",
    "                 'outdoor_space', 'dining_room', 'high_speed_internet', 'balcony',\n",
    "                 'swimming_pool', 'new_construction', 'exclusive', 'terrace', \n",
    "                 'loft', 'garden_patio', 'common_outdoor_space', \n",
    "                 'wheelchair_access']\n",
    "    X['perk_count'] = X[perk_cols].sum(axis=1)\n",
    "\n",
    "    # Are cats or dogs allowed?\n",
    "    X['cats_or_dogs'] = (X['cats_allowed']==1) | (X['dogs_allowed']==1)\n",
    "\n",
    "    # Are cats and dogs allowed?\n",
    "    X['cats_and_dogs'] = (X['cats_allowed']==1) & (X['dogs_allowed']==1)\n",
    "\n",
    "    # Total number of rooms (beds + baths)\n",
    "    X['rooms'] = X['bedrooms'] + X['bathrooms']\n",
    "\n",
    "    return X\n",
    "\n",
    "X_train = engineer_features(X_train)\n",
    "X_test = engineer_features(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Could we try every possible feature combination?\n",
    "\n",
    "The number of [combinations](https://en.wikipedia.org/wiki/Combination) is shocking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many features do we have currently?\n",
    "features = X_train.columns\n",
    "n = len(features)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many ways to choose 1 to n features?\n",
    "from math import factorial\n",
    "\n",
    "def n_choose_k(n, k):\n",
    "    return factorial(n)/(factorial(k)*factorial(n-k))\n",
    "\n",
    "combinations = sum(n_choose_k(n,k) for k in range(1,n+1))\n",
    "print(f'{combinations:,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't try every possible combination, but we can try some. For example, we can use univariate statistical tests to measure the correlation between each feature and the target, and select the k best features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the [Scikit-Learn User Guide on Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Select the 15 features that best correlate with the target\n",
    "# (15 is an arbitrary starting point here)\n",
    "\n",
    "# SelectKBest has a similar API to what we've seen before.\n",
    "# IMPORTANT!\n",
    "# .fit_transform on the train set\n",
    "# .transform on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Which features were selected?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: How many features should be selected?\n",
    "\n",
    "# You can try a range of values for k,\n",
    "# then choose the model with the best score.\n",
    "# If multiple models \"tie\" for the best score,\n",
    "# choose the simplest model.\n",
    "# You decide what counts as a tie!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "In your assignment, you will do feature selection with SelectKBest.\n",
    "\n",
    "You'll go back to our other New York City real estate dataset. Instead of predicting apartment rents, you'll predict property sales prices. But not just for condos in Tribeca. Instead, you'll predict property sales prices for One Family Dwellings, where the sale price was more than 100 thousand and less than 2 million.\n",
    "\n",
    "If you run the above code cell with your assignment dataset, you will probably get some shocking results like these:\n",
    "\n",
    "```\n",
    "1 features\n",
    "Test MAE: $183,641 \n",
    "\n",
    "2 features\n",
    "Test MAE: $182,569 \n",
    "\n",
    "3 features\n",
    "Test MAE: $182,569 \n",
    "\n",
    "4 features\n",
    "Test MAE: $183,441 \n",
    "\n",
    "5 features\n",
    "Test MAE: $186,532 \n",
    "\n",
    "6 features\n",
    "Test MAE: $176,121 \n",
    "\n",
    "7 features\n",
    "Test MAE: $177,001 \n",
    "\n",
    "8 features\n",
    "Test MAE: $176,707 \n",
    "\n",
    "9 features\n",
    "Test MAE: $170,969 \n",
    "\n",
    "10 features\n",
    "Test MAE: $170,977 \n",
    "\n",
    "11 features\n",
    "Test MAE: $170,507 \n",
    "\n",
    "12 features\n",
    "Test MAE: $162,301 \n",
    "\n",
    "13 features\n",
    "Test MAE: $163,559 \n",
    "\n",
    "14 features\n",
    "Test MAE: $162,562 \n",
    "\n",
    "15 features\n",
    "Test MAE: $162,550 \n",
    "\n",
    "16 features\n",
    "Test MAE: $162,678 \n",
    "\n",
    "17 features\n",
    "Test MAE: $162,419 \n",
    "\n",
    "18 features\n",
    "Test MAE: $162,177 \n",
    "\n",
    "19 features\n",
    "Test MAE: $162,177 \n",
    "\n",
    "20 features\n",
    "Test MAE: $157,893 \n",
    "\n",
    "21 features\n",
    "Test MAE: $157,966 \n",
    "\n",
    "22 features\n",
    "Test MAE: $157,966 \n",
    "\n",
    "23 features\n",
    "Test MAE: $157,966 \n",
    "\n",
    "24 features\n",
    "Test MAE: $157,630 \n",
    "\n",
    "25 features\n",
    "Test MAE: $157,580 \n",
    "\n",
    "26 features\n",
    "Test MAE: $25,968,990,575,776,280 \n",
    "\n",
    "27 features\n",
    "Test MAE: $157,550 \n",
    "\n",
    "28 features\n",
    "Test MAE: $87,300,193,986,380,608 \n",
    "\n",
    "29 features\n",
    "Test MAE: $158,491 \n",
    "\n",
    "30 features\n",
    "Test MAE: $17,529,140,644,990,770 \n",
    "\n",
    "31 features\n",
    "Test MAE: $24,191,458,933,688,856 \n",
    "\n",
    "32 features\n",
    "Test MAE: $15,214,122,471,992,104 \n",
    "\n",
    "33 features\n",
    "Test MAE: $15,539,731,847,001,626 \n",
    "\n",
    "34 features\n",
    "Test MAE: $26,823,915,969,200,480 \n",
    "\n",
    "35 features\n",
    "Test MAE: $3,813,290,272,870,121 \n",
    "\n",
    "36 features\n",
    "Test MAE: $157,900 \n",
    "\n",
    "37 features\n",
    "Test MAE: $157,900 \n",
    "\n",
    "38 features\n",
    "Test MAE: $158,911 \n",
    "\n",
    "39 features\n",
    "Test MAE: $9,101,846,282,581,472 \n",
    "\n",
    "40 features\n",
    "Test MAE: $1,424,168,120,777,820 \n",
    "\n",
    "41 features\n",
    "Test MAE: $158,261 \n",
    "\n",
    "42 features\n",
    "Test MAE: $158,261 \n",
    "\n",
    "43 features\n",
    "Test MAE: $4,784,333,158,313,152 \n",
    "\n",
    "44 features\n",
    "Test MAE: $1,329,759,264,341,476 \n",
    "\n",
    "45 features\n",
    "Test MAE: $158,451 \n",
    "\n",
    "46 features\n",
    "Test MAE: $158,450 \n",
    "\n",
    "47 features\n",
    "Test MAE: $158,450 \n",
    "\n",
    "48 features\n",
    "Test MAE: $1,331,383,815,682,658 \n",
    "\n",
    "49 features\n",
    "Test MAE: $1,504,319,420,789,134 \n",
    "\n",
    "50 features\n",
    "Test MAE: $2,285,927,437,866,492\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did the error blow up with 26 features? We can look at the coefficients of the selected features:\n",
    "\n",
    "```\n",
    "BLOCK                                                  -97,035\n",
    "ZIP_CODE                                              -152,985\n",
    "COMMERCIAL_UNITS                    -4,115,324,664,197,034,496\n",
    "TOTAL_UNITS                         -2,872,516,607,892,124,160\n",
    "GROSS_SQUARE_FEET                                      123,739\n",
    "BOROUGH_3                                              146,876\n",
    "BOROUGH_4                                              197,262\n",
    "BOROUGH_2                                              -55,917\n",
    "BOROUGH_5                                              -84,107\n",
    "NEIGHBORHOOD_OTHER                                      56,036\n",
    "NEIGHBORHOOD_FLUSHING-NORTH                             63,394\n",
    "NEIGHBORHOOD_FOREST HILLS                               46,411\n",
    "NEIGHBORHOOD_BOROUGH PARK                               29,915\n",
    "TAX_CLASS_AT_PRESENT_1D               -545,831,543,721,722,112\n",
    "BUILDING_CLASS_AT_PRESENT_A5                            -1,673\n",
    "BUILDING_CLASS_AT_PRESENT_A3             5,516,361,218,128,626\n",
    "BUILDING_CLASS_AT_PRESENT_S1         1,735,885,668,110,473,216\n",
    "BUILDING_CLASS_AT_PRESENT_A6           -25,974,113,243,185,788\n",
    "BUILDING_CLASS_AT_PRESENT_A8          -760,608,646,332,801,664\n",
    "BUILDING_CLASS_AT_PRESENT_S0           941,854,072,479,442,176\n",
    "BUILDING_CLASS_AT_TIME_OF_SALE_A5                      -24,599\n",
    "BUILDING_CLASS_AT_TIME_OF_SALE_A3       -5,516,361,218,111,506\n",
    "BUILDING_CLASS_AT_TIME_OF_SALE_S1    4,253,055,231,847,939,072\n",
    "BUILDING_CLASS_AT_TIME_OF_SALE_A6       25,974,113,243,176,404\n",
    "BUILDING_CLASS_AT_TIME_OF_SALE_A8     -541,733,439,448,260,800\n",
    "BUILDING_CLASS_AT_TIME_OF_SALE_S0      990,851,394,820,416,512\n",
    "```\n",
    "\n",
    "These were the coefficients that minimized the sum of squared errors in the training set. But this model has become too complex, with extreme coefficients that can lead to extreme predictions on new unseen data. \n",
    "\n",
    "This linear model needs _regularization._ Ridge Regression to the rescue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use scikit-learn to fit Ridge Regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Josh Starmer explains in his [StatQuest video on Ridge Regression:](https://youtu.be/Q81RR3yKn30?t=222)\n",
    "\n",
    "> The main idea behind **Ridge Regression** is to find a new line that _doesn't_ fit the training data as well. In other words, we introduce a small amount of **bias** into how the new line is fit to the data. But in return for that small amount of bias we get a significant drop in **variance.** In other words, by starting with a slightly worse fit Ridge regression can provide better long-term predictions. BAM!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS vs Mean Baseline vs Ridge\n",
    "\n",
    "Let's see look at some examples.\n",
    "\n",
    "First, here's a famous, tiny dataset — [Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet), dataset III:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "anscombe = sns.load_dataset('anscombe').query('dataset==\"III\"')\n",
    "anscombe.plot.scatter('x', 'y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll compare:\n",
    "- Ordinary Least Squares Linear Regression\n",
    "- Mean Baseline\n",
    "- Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Plot data\n",
    "ax = anscombe.plot.scatter('x', 'y')\n",
    "\n",
    "# Fit linear model\n",
    "ols = LinearRegression()\n",
    "ols.fit(anscombe[['x']], anscombe['y'])\n",
    "\n",
    "# Get linear equation\n",
    "m = ols.coef_[0].round(2)\n",
    "b = ols.intercept_.round(2)\n",
    "title = f'Linear Regression \\n y = {m}x + {b}'\n",
    "\n",
    "# Get predictions\n",
    "anscombe['y_pred'] = ols.predict(anscombe[['x']])\n",
    "\n",
    "# Plot predictions\n",
    "anscombe.plot('x', 'y_pred', ax=ax, title=title);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "ax = anscombe.plot.scatter('x', 'y')\n",
    "\n",
    "# Mean baseline\n",
    "mean = anscombe['y'].mean()\n",
    "anscombe['y_pred'] = mean\n",
    "title = f'Mean Baseline \\n y = 0x + {mean:.2f}'\n",
    "\n",
    "# Plot \"predictions\"\n",
    "anscombe.plot('x', 'y_pred', ax=ax, title=title);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression\n",
    "\n",
    "With increasing regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def ridge_anscombe(alpha):\n",
    "    \"\"\"\n",
    "    Fit & plot a ridge regression model,\n",
    "    with Anscombe Quartet dataset III.\n",
    "\n",
    "    alpha : positive float, regularization strength\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data\n",
    "    anscombe = sns.load_dataset('anscombe').query('dataset==\"III\"')\n",
    "\n",
    "    # Plot data\n",
    "    ax = anscombe.plot.scatter('x', 'y')\n",
    "\n",
    "    # Fit linear model\n",
    "    ridge = Ridge(alpha=alpha, normalize=True)\n",
    "    ridge.fit(anscombe[['x']], anscombe['y'])\n",
    "\n",
    "    # Get linear equation\n",
    "    m = ridge.coef_[0].round(2)\n",
    "    b = ridge.intercept_.round(2)\n",
    "    title = f'Ridge Regression, alpha={alpha} \\n y = {m}x + {b}'\n",
    "\n",
    "    # Get predictions\n",
    "    anscombe['y_pred'] = ridge.predict(anscombe[['x']])\n",
    "\n",
    "    # Plot predictions\n",
    "    anscombe.plot('x', 'y_pred', ax=ax, title=title)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "for alpha in range(10):\n",
    "    ridge_anscombe(alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the Ridge Regression has no regularization (`alpha=0`) then it is identical to Ordinary Least Squares Regression.\n",
    "\n",
    "When we increase the regularization, the predictions looks less and less like OLS and more and more like the mean baseline. The predictions are less sensitive to changes in the independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may ask, how should we decide the amount of regularization?\n",
    "\n",
    "[The StatQuest video answers,](https://youtu.be/Q81RR3yKn30?t=602)\n",
    "\n",
    "> So how do we decide what value to give lambda? We just try a bunch of values for lambda, and use cross-validation\n",
    "typically 10-fold cross-validation, to determine which one results in the lowest variance. DOUBLE BAM!!!\n",
    "\n",
    "You'll learn more about cross-validation next sprint. For now, the good news is scikit-learn gives us [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html), \"Ridge regression with built-in cross-validation.\"\n",
    "\n",
    "Also, notice that scikit-learn calls the regularization parameter \"alpha\", but StatQuest calls it \"lambda.\" The greek letters are different, but the concept is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try these values for alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.1, 1.0, 10.0, 100.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) to find the best alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "ridge = RidgeCV(alphas=alphas, normalize=True)\n",
    "ridge.fit(anscombe[['x']], anscombe['y'])\n",
    "ridge.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit looks similar to Ordinary Least Squares Regression, but slightly less influenced by the outlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "ax = anscombe.plot.scatter('x', 'y')\n",
    "\n",
    "# Get linear equation\n",
    "m = ridge.coef_[0].round(2)\n",
    "b = ridge.intercept_.round(2)\n",
    "title = f'Ridge Regression, alpha={ridge.alpha_} \\n y = {m}x + {b}'\n",
    "\n",
    "# Get predictions\n",
    "anscombe['y_pred'] = ridge.predict(anscombe[['x']])\n",
    "\n",
    "# Plot predictions\n",
    "anscombe.plot('x', 'y_pred', ax=ax, title=title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYC, 1 feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our other New York City dataset, to demonstrate regularization with Ridge Regresson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Try a range of alpha parameters for Ridge Regression.\n",
    "\n",
    "# The scikit-learn docs explain, \n",
    "# alpha : Regularization strength; must be a positive float. Regularization \n",
    "# improves the conditioning of the problem and reduces the variance of the \n",
    "# estimates. Larger values specify stronger regularization.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "for alpha in [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]:\n",
    "    \n",
    "    # Fit Ridge Regression model\n",
    "    feature = 'bedrooms'\n",
    "    display(HTML(f'Ridge Regression, with alpha={alpha}'))\n",
    "    model = Ridge(alpha=alpha, normalize=True)\n",
    "    model.fit(X_train[[feature]], y_train)\n",
    "    \n",
    "    # Get Test MAE\n",
    "    y_pred = model.predict(X_test[[feature]])\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    display(HTML(f'Test Mean Absolute Error: ${mae:,.0f}'))\n",
    "    \n",
    "    train.plot.scatter(feature, target, alpha=0.05)\n",
    "    plt.plot(X_test[feature], y_pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYC, multiple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for alpha in [0.001, 0.01, 0.1, 1.0, 1, 100.0, 1000.0]:\n",
    "    \n",
    "    # Fit Ridge Regression model\n",
    "    display(HTML(f'Ridge Regression, with alpha={alpha}'))\n",
    "    model = Ridge(alpha=alpha, normalize=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Get Test MAE\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    display(HTML(f'Test Mean Absolute Error: ${mae:,.0f}'))\n",
    "    \n",
    "    # Plot coefficients\n",
    "    coefficients = pd.Series(model.coef_, X_train.columns)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    coefficients.sort_values().plot.barh(color='grey')\n",
    "    plt.xlim(-400,700)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization just means \"add bias\"\n",
    "\n",
    "OK, there's a bit more to it than that. But that's the core intuition - the problem is the model working \"too well\", so fix it by making it harder for the model!\n",
    "\n",
    "It may sound strange - a technique that is purposefully \"worse\" - but in certain situations, it can really get results.\n",
    "\n",
    "What's bias? In the context of statistics and machine learning, bias is when a predictive model fails to identify relationships between features and the output. In a word, bias is *underfitting*.\n",
    "\n",
    "We want to add bias to the model because of the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) - variance is the sensitivity of a model to the random noise in its training data (i.e. *overfitting*), and bias and variance are naturally (inversely) related. Increasing one will always decrease the other, with regards to the overall generalization error (predictive accuracy on unseen data).\n",
    "\n",
    "Visually, the result looks like this:\n",
    "\n",
    "![Regularization example plot](https://upload.wikimedia.org/wikipedia/commons/0/02/Regularization.svg)\n",
    "\n",
    "The blue line is overfit, using more dimensions than are needed to explain the data and so much of the movement is based on noise and won't generalize well. The green line still fits the data, but is less susceptible to the noise - depending on how exactly we parameterize \"noise\" we may throw out actual correlation, but if we balance it right we keep that signal and greatly improve generalizability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "In your assignment, you will fit a Ridge Regression model. You can try Linear Regression too — depending on how many features you select, your errors will probably blow up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "For your assignment, we're going back to our other **New York City** real estate dataset. Instead of predicting apartment rents, you'll predict property sales prices.\n",
    "\n",
    "But not just for condos in Tribeca...\n",
    "\n",
    "Instead, predict property sales prices for **One Family Dwellings** (`BUILDING_CLASS_CATEGORY` == `'01 ONE FAMILY DWELLINGS'`). \n",
    "\n",
    "Use a subset of the data where the **sale price was more than \\\\$100 thousand and less than $2 million.** \n",
    "\n",
    "You'll practice all of the module's objectives to build your best model yet!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
