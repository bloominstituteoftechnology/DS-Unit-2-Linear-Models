{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BloomTech Data Science\n",
    "\n",
    "*Unit 2, Sprint 1, Module 4*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Project: Logistic Regression\n",
    "\n",
    "Do you like burritos? ðŸŒ¯ You're in luck then, because in this project you'll create a model to predict whether a burrito is `'Great'`.\n",
    "\n",
    "The dataset for this assignment comes from [Scott Cole](https://srcole.github.io/100burritos/), a San Diego-based data scientist and burrito enthusiast. \n",
    "\n",
    "## Directions\n",
    "\n",
    "The tasks for this project are the following:\n",
    "\n",
    "- **Task 1:** Import `csv` file using `wrangle` function.\n",
    "- **Task 2:** Conduct exploratory data analysis (EDA), and modify `wrangle` function .\n",
    "- **Task 3:** Split data into feature matrix `X` and target vector `y`.\n",
    "- **Task 4:** Split feature matrix `X` and target vector `y` into training and test sets.\n",
    "- **Task 5:** Establish the baseline accuracy score for your dataset.\n",
    "- **Task 6:** Build `model_logr` using a pipeline that includes three transfomers and `LogisticRegression` predictor. Train model on `X_train` and `X_test`.\n",
    "- **Task 7:** Calculate the training and test accuracy score for your model.\n",
    "- **Task 8:** Create a horizontal bar chart showing the 10 most influencial features for your  model. \n",
    "- **Task 9:** Demonstrate and explain the differences between `model_lr.predict()` and `model_lr.predict_proba()`.\n",
    "\n",
    "**Note** \n",
    "\n",
    "You should limit yourself to the following libraries:\n",
    "\n",
    "- `category_encoders`\n",
    "- `matplotlib`\n",
    "- `pandas`\n",
    "- `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Wrangle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T21:02:20.394062Z",
     "start_time": "2023-10-06T21:02:20.118891Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T21:02:20.397217Z",
     "start_time": "2023-10-06T21:02:20.395159Z"
    }
   },
   "outputs": [],
   "source": [
    "def wrangle(filepath):\n",
    "    # Import w/ DateTimeIndex\n",
    "    df = pd.read_csv(filepath, parse_dates=['Date'],\n",
    "                     index_col='Date')\n",
    "    \n",
    "    # Drop unrated burritos\n",
    "    df.dropna(subset=['overall'], inplace=True)\n",
    "    \n",
    "    # Derive binary classification target:\n",
    "    # We define a 'Great' burrito as having an\n",
    "    # overall rating of 4 or higher, on a 5 point scale\n",
    "    df['Great'] = (df['overall'] >= 4).astype(int)\n",
    "    \n",
    "    # Drop high cardinality categoricals\n",
    "    df = df.drop(columns=['Notes', 'Location', 'Address', 'URL', 'Neighborhood'])\n",
    "    \n",
    "    # Drop columns to prevent \"leakage\"\n",
    "    df = df.drop(columns=['Rec', 'overall'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** Use the above `wrangle` function to import the `burritos.csv` file into a DataFrame named `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T21:02:26.208748Z",
     "start_time": "2023-10-06T21:02:25.881364Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                  Burrito  Yelp  Google Chips  Cost  Hunger  Mass (g)  \\\nDate                                                                    \n2016-01-18    California    3.5     4.2   NaN  6.49     3.0       NaN   \n2016-01-24    California    3.5     3.3   NaN  5.45     3.5       NaN   \n2016-01-24       Carnitas   NaN     NaN   NaN  4.85     1.5       NaN   \n2016-01-24    Carne asada   NaN     NaN   NaN  5.25     2.0       NaN   \n2016-01-27     California   4.0     3.8     x  6.59     4.0       NaN   \n...                   ...   ...     ...   ...   ...     ...       ...   \n2019-08-27      Al Pastor   NaN     NaN   NaN  6.00     1.0       NaN   \n2019-08-27  Chile Relleno   NaN     NaN   NaN  6.00     4.0       NaN   \n2019-08-27     California   NaN     NaN   NaN  7.90     3.0       NaN   \n2019-08-27         Shrimp   NaN     NaN   NaN  7.90     3.0       NaN   \n2019-08-27    Pollo Asado   NaN     NaN   NaN  5.50     3.5       NaN   \n\n            Density (g/mL)  Length  Circum  ...  Lobster  Queso  Egg  \\\nDate                                        ...                        \n2016-01-18             NaN     NaN     NaN  ...      NaN    NaN  NaN   \n2016-01-24             NaN     NaN     NaN  ...      NaN    NaN  NaN   \n2016-01-24             NaN     NaN     NaN  ...      NaN    NaN  NaN   \n2016-01-24             NaN     NaN     NaN  ...      NaN    NaN  NaN   \n2016-01-27             NaN     NaN     NaN  ...      NaN    NaN  NaN   \n...                    ...     ...     ...  ...      ...    ...  ...   \n2019-08-27             NaN    17.0    20.5  ...      NaN    NaN  NaN   \n2019-08-27             NaN    19.0    26.0  ...      NaN    NaN  NaN   \n2019-08-27             NaN    20.0    22.0  ...      NaN    NaN  NaN   \n2019-08-27             NaN    22.5    24.5  ...      NaN    NaN  NaN   \n2019-08-27             NaN    17.0    21.3  ...      NaN    NaN  NaN   \n\n            Mushroom  Bacon  Sushi  Avocado  Corn  Zucchini  Great  \nDate                                                                \n2016-01-18       NaN    NaN    NaN      NaN   NaN       NaN      0  \n2016-01-24       NaN    NaN    NaN      NaN   NaN       NaN      0  \n2016-01-24       NaN    NaN    NaN      NaN   NaN       NaN      0  \n2016-01-24       NaN    NaN    NaN      NaN   NaN       NaN      0  \n2016-01-27       NaN    NaN    NaN      NaN   NaN       NaN      1  \n...              ...    ...    ...      ...   ...       ...    ...  \n2019-08-27       NaN    NaN    NaN      NaN   NaN       NaN      0  \n2019-08-27       NaN    NaN    NaN      NaN   NaN       NaN      1  \n2019-08-27       NaN    NaN    NaN      NaN   NaN       NaN      0  \n2019-08-27       NaN    NaN    NaN      NaN   NaN       NaN      1  \n2019-08-27       NaN    NaN    NaN      NaN   NaN       NaN      1  \n\n[421 rows x 59 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Burrito</th>\n      <th>Yelp</th>\n      <th>Google</th>\n      <th>Chips</th>\n      <th>Cost</th>\n      <th>Hunger</th>\n      <th>Mass (g)</th>\n      <th>Density (g/mL)</th>\n      <th>Length</th>\n      <th>Circum</th>\n      <th>...</th>\n      <th>Lobster</th>\n      <th>Queso</th>\n      <th>Egg</th>\n      <th>Mushroom</th>\n      <th>Bacon</th>\n      <th>Sushi</th>\n      <th>Avocado</th>\n      <th>Corn</th>\n      <th>Zucchini</th>\n      <th>Great</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2016-01-18</th>\n      <td>California</td>\n      <td>3.5</td>\n      <td>4.2</td>\n      <td>NaN</td>\n      <td>6.49</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2016-01-24</th>\n      <td>California</td>\n      <td>3.5</td>\n      <td>3.3</td>\n      <td>NaN</td>\n      <td>5.45</td>\n      <td>3.5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2016-01-24</th>\n      <td>Carnitas</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.85</td>\n      <td>1.5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2016-01-24</th>\n      <td>Carne asada</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.25</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2016-01-27</th>\n      <td>California</td>\n      <td>4.0</td>\n      <td>3.8</td>\n      <td>x</td>\n      <td>6.59</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2019-08-27</th>\n      <td>Al Pastor</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.00</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>17.0</td>\n      <td>20.5</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2019-08-27</th>\n      <td>Chile Relleno</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.00</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>19.0</td>\n      <td>26.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2019-08-27</th>\n      <td>California</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.90</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>20.0</td>\n      <td>22.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2019-08-27</th>\n      <td>Shrimp</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.90</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>22.5</td>\n      <td>24.5</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2019-08-27</th>\n      <td>Pollo Asado</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.50</td>\n      <td>3.5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>17.0</td>\n      <td>21.3</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>421 rows Ã— 59 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"https://raw.githubusercontent.com/bloominstituteoftechnology/DS-Unit-2-Linear-Models/master/data/burritos/burritos.csv\"\n",
    "df = wrangle(filepath)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During your exploratory data analysis, note that there are several columns whose data type is `object` but that seem to be a binary encoding. For example, `df['Beef'].head()` returns:\n",
    "\n",
    "```\n",
    "0      x\n",
    "1      x\n",
    "2    NaN\n",
    "3      x\n",
    "4      x\n",
    "Name: Beef, dtype: object\n",
    "```\n",
    "\n",
    "**Task 2:** Change the `wrangle` function so that these columns are properly encoded as `0` and `1`s. Be sure your code handles upper- and lowercase `X`s, and `NaN`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct your exploratory data analysis here\n",
    "# And modify the `wrangle` function above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you explore the `'Burrito'` column of `df`, you'll notice that it's a high-cardinality categorical feature. You'll also notice that there's a lot of overlap between the categories. \n",
    "\n",
    "**Stretch Goal:** Change the `wrangle` function above so that it engineers four new features: `'california'`, `'asada'`, `'surf'`, and `'carnitas'`. Each row should have a `1` or `0` based on the text information in the `'Burrito'` column. For example, here's how the first 5 rows of the dataset would look.\n",
    "\n",
    "| **Burrito** | **california** | **asada** | **surf** | **carnitas** |\n",
    "| :---------- | :------------: | :-------: | :------: | :----------: |\n",
    "| California  |       1        |     0     |    0     |      0       |\n",
    "| California  |       1        |     0     |    0     |      0       |\n",
    "|  Carnitas   |       0        |     0     |    0     |      1       |\n",
    "| Carne asada |       0        |     1     |    0     |      0       |\n",
    "| California  |       1        |     0     |    0     |      0       |\n",
    "\n",
    "**Note:** Be sure to also drop the `'Burrito'` once you've engineered your new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct your exploratory data analysis here\n",
    "# And modify the `wrangle` function above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Split Data\n",
    "\n",
    "**Task 3:** Split your dataset into the feature matrix `X` and the target vector `y`. You want to predict `'Great'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ...\n",
    "y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** Split `X` and `y` into a training set (`X_train`, `y_train`) and a test set (`X_test`, `y_test`).\n",
    "\n",
    "- Your training set should include data from 2016 through 2017. \n",
    "- Your test set should include data from 2018 and later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = ..., ...\n",
    "X_test, y_test = ..., ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Establish Baseline\n",
    "\n",
    "**Task 5:** Since this is a **classification** problem, you should establish a baseline accuracy score. Figure out what is the majority class in `y_train` and what percentage of your training observations it represents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_acc = ...\n",
    "print('Baseline Accuracy Score:', baseline_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Build Model\n",
    "\n",
    "**Task 6:** Build a `Pipeline` named `model_logr`, and fit it to your training data. Your pipeline should include:\n",
    "\n",
    "- a `OneHotEncoder` transformer for categorical features, \n",
    "- a `SimpleImputer` transformer to deal with missing values, \n",
    "- a [`StandarScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) transfomer (which often improves performance in a logistic regression model), and \n",
    "- a `LogisticRegression` predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logr = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Check Metrics\n",
    "\n",
    "**Task 7:** Calculate the training and test accuracy score for `model_logr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_acc = ...\n",
    "test_acc = ...\n",
    "\n",
    "print('Training MAE:', training_acc)\n",
    "print('Test MAE:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Communicate Results\n",
    "\n",
    "**Task 8:** Create a horizontal barchart that plots the 10 most important coefficients for `model_lr`, sorted by absolute value.\n",
    "\n",
    "**Note:** Since you created your model using a `Pipeline`, you'll need to use the [`named_steps`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) attribute to access the coefficients in your `LogisticRegression` predictor. Be sure to look at the shape of the coefficients array before you combine it with the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your horizontal barchart here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more than one way to generate predictions with `model_lr`. For instance, you can use [`predict`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression) or [`predict_proba`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression.predict_proba).\n",
    "\n",
    "**Task 9:** Generate predictions for `X_test` using both `predict` and `predict_proba`. Then below, write a summary of the differences in the output for these two methods. You should answer the following questions:\n",
    "\n",
    "- What data type do `predict` and `predict_proba` output?\n",
    "- What are the shapes of their different output?\n",
    "- What numerical values are in the output?\n",
    "- What do those numerical values represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here to explore the differences between `predict` and `predict_proba`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give your written answer here:**\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_214_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
