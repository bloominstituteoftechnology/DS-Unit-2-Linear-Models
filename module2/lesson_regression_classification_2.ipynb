{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science, Unit 2: Predictive Modeling\n",
    "\n",
    "# Regression & Classification, Module 2\n",
    "\n",
    "## Objectives\n",
    "- Go from simple regression (1 feature) to multiple regression (2+ features)\n",
    "- Use regression metrics: MAE (Mean Absolute Error), RMSE (Root Mean Squared Error), and $R^2$ Score\n",
    "- Understand how ordinary least squares regression minimizes the sum of squared errors\n",
    "- Get and plot coefficients\n",
    "- Explain why overfitting is a problem. Do train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're in Colab...\n",
    "import os, sys\n",
    "in_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if in_colab:\n",
    "    # Install required python packages:\n",
    "    # pandas-profiling, version >= 2.0\n",
    "    # plotly, version >= 4.0\n",
    "    !pip install --upgrade pandas-profiling plotly\n",
    "    \n",
    "    # Pull files from Github repo\n",
    "    os.chdir('/content')\n",
    "    !git init .\n",
    "    !git remote add origin https://github.com/LambdaSchool/DS-Unit-2-Regression-Classification.git\n",
    "    !git pull origin master\n",
    "    \n",
    "    # Change into directory for module\n",
    "    os.chdir('module2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this Numpy warning when using Plotly Express:\n",
    "# FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning, module='numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these functions later\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def regression_3d(df, x, y, z, **kwargs):\n",
    "    \"\"\"\n",
    "    Visualize linear regression in 3D: 2 features + 1 target\n",
    "    \n",
    "    df : Pandas DataFrame\n",
    "    x : string, feature 1 column in df\n",
    "    y : string, feature 2 column in df\n",
    "    z : string, target column in df\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plot data\n",
    "    fig = px.scatter_3d(df, x, y, z, **kwargs)\n",
    "    \n",
    "    # Fit Linear Regression\n",
    "    features = [x, y]\n",
    "    target = z\n",
    "    model = LinearRegression()\n",
    "    model.fit(df[features], df[target])    \n",
    "    \n",
    "    # Define grid of four points in the feature space\n",
    "    xmin, xmax = df[x].min(), df[x].max()\n",
    "    ymin, ymax = df[y].min(), df[y].max()\n",
    "    coords = [[xmin, ymin], \n",
    "              [xmin, ymax], \n",
    "              [xmax, ymin], \n",
    "              [xmax, ymax]]\n",
    "    \n",
    "    # Make predictions for the grid\n",
    "    Z = model.predict(coords).reshape((2,2), order='F')\n",
    "    \n",
    "    # Plot predictions as a 3D surface (plane)\n",
    "    fig.add_trace(go.Surface(x=[xmin,xmax], y=[ymin,ymax], z=Z))\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def regression_residuals(df, feature, target, m, b):\n",
    "    \"\"\"\n",
    "    Visualize linear regression, with residual errors,\n",
    "    in 2D: 1 feature + 1 target.\n",
    "    \n",
    "    Use the m & b parameters to \"fit the model\" manually.\n",
    "    \n",
    "    df : Pandas DataFrame\n",
    "    feature : string, feature column in df\n",
    "    target : string, target column in df\n",
    "    m : numeric, slope for linear equation\n",
    "    b : numeric, intercept for linear requation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plot data\n",
    "    df.plot.scatter(feature, target)\n",
    "    \n",
    "    # Make predictions\n",
    "    x = df[feature]\n",
    "    y = df[target]\n",
    "    y_pred = m*x + b\n",
    "    \n",
    "    # Plot predictions\n",
    "    plt.plot(x, y_pred)\n",
    "    \n",
    "    # Plot residual errors\n",
    "    for x, y1, y2 in zip(x, y, y_pred):\n",
    "        plt.plot((x, x), (y1, y2), color='grey')\n",
    "    \n",
    "    # Print regression metrics\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    print('Mean Absolute Error:', mae)\n",
    "    print('R^2:', r2)\n",
    "\n",
    "\n",
    "def regression_squared_errors(df, feature, target, m, b):\n",
    "    \"\"\"\n",
    "    Visualize linear regression, with squared errors,\n",
    "    in 2D: 1 feature + 1 target.\n",
    "    \n",
    "    Use the m & b parameters to \"fit the model\" manually.\n",
    "    \n",
    "    df : Pandas DataFrame\n",
    "    feature : string, feature column in df\n",
    "    target : string, target column in df\n",
    "    m : numeric, slope for linear equation\n",
    "    b : numeric, intercept for linear requation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plot data\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    ax = plt.axes()\n",
    "    df.plot.scatter(feature, target, ax=ax)\n",
    "    \n",
    "    # Make predictions\n",
    "    x = df[feature]\n",
    "    y = df[target]\n",
    "    y_pred = m*x + b\n",
    "    \n",
    "    # Plot predictions\n",
    "    ax.plot(x, y_pred)\n",
    "    \n",
    "    # Plot squared errors\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    scale = (xmax-xmin)/(ymax-ymin)\n",
    "    for x, y1, y2 in zip(x, y, y_pred):\n",
    "        bottom_left = (x, min(y1, y2))\n",
    "        height = abs(y1 - y2)\n",
    "        width = height * scale\n",
    "        ax.add_patch(Rectangle(xy=bottom_left, width=width, height=height, alpha=0.1))\n",
    "    \n",
    "    # Print regression metrics\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    print('Mean Squared Error:', mse)\n",
    "    print('Root Mean Squared Error:', rmse)\n",
    "    print('Mean Absolute Error:', mae)\n",
    "    print('R^2:', r2)\n",
    "    \n",
    "\n",
    "\n",
    "# Credit: Jake VanderPlas, Python Data Science Handbook, Chapter 5.3\n",
    "# https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#Validation-curves-in-Scikit-Learn\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), \n",
    "                         LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Elections! ðŸ—³ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "df = pd.read_csv('../data/bread_peace_voting.csv')\n",
    "px.scatter(\n",
    "    df,\n",
    "    x='Average Recent Growth in Personal Incomes',\n",
    "    y='Incumbent Party Vote Share',\n",
    "    text='Year',\n",
    "    title='US Presidential Elections, 1952-2016',\n",
    "    trendline='ols',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Douglas Hibbs, [Background Information on the â€˜Bread and Peaceâ€™ Model of Voting in Postwar US Presidential Elections](https://douglas-hibbs.com/background-information-on-bread-and-peace-voting-in-us-presidential-elections/)\n",
    "\n",
    "> Aggregate two-party vote shares going to candidates of the party holding the presidency during the postwar era are well explained by just two fundamental determinants:\n",
    "\n",
    "> (1) Positively by weighted-average growth of per capita real disposable personal income over the term.  \n",
    "> (2) Negatively by cumulative US military fatalities (scaled to population) owing to unprovoked, hostile deployments of American armed forces in foreign wars.\n",
    "\n",
    "#### Data sources\n",
    "- 1952-2012: Douglas Hibbs, [2014 lecture at Deakin University Melbourne](http://www.douglas-hibbs.com/HibbsArticles/HIBBS-PRESVOTE-SLIDES-MELBOURNE-Part1-2014-02-26.pdf), Slide 40\n",
    "- 2016, Vote Share: [The American Presidency Project](https://www.presidency.ucsb.edu/statistics/elections)\n",
    "- 2016, Recent Growth in Personal Incomes: [The 2016 election economy: the \"Bread and Peace\" model final forecast](https://angrybearblog.com/2016/11/the-2016-election-economy-the-bread-and-peace-model-final-forecast.html)\n",
    "- 2016, US Military Fatalities: Assumption that Afghanistan War fatalities in 2012-16 occured at the same rate as 2008-12\n",
    "\n",
    "> Fatalities denotes the cumulative number of American military fatalities per millions of US population the in Korea, Vietnam, Iraq and Afghanistan wars during the presidential terms preceding the 1952, 1964, 1968, 1976 and 2004, 2008 and 2012 elections. â€”[Hibbs](http://www.douglas-hibbs.com/HibbsArticles/HIBBS-PRESVOTE-SLIDES-MELBOURNE-Part1-2014-02-26.pdf), Slide 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "\n",
    "The same result that is found by minimizing the sum of the squared errors can be also found through a linear algebra process known as the \"Least Squares Solution:\"\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\beta} = (X^{T}X)^{-1}X^{T}y\n",
    "\\end{align}\n",
    "\n",
    "Before we can work with this equation in its linear algebra form we have to understand how to set up the matrices that are involved in this equation. \n",
    "\n",
    "### The $\\beta$ vector\n",
    "\n",
    "The $\\beta$ vector represents all the parameters that we are trying to estimate, our $y$ vector and $X$ matrix values are full of data from our dataset. The $\\beta$ vector holds the variables that we are solving for: $\\beta_0$ and $\\beta_1$\n",
    "\n",
    "Now that we have all of the necessary parts we can set them up in the following equation:\n",
    "\n",
    "\\begin{align}\n",
    "y = X \\beta + \\epsilon\n",
    "\\end{align}\n",
    "\n",
    "Since our $\\epsilon$ value represents **random** error we can assume that it will equal zero on average.\n",
    "\n",
    "\\begin{align}\n",
    "y = X \\beta\n",
    "\\end{align}\n",
    "\n",
    "The objective now is to isolate the $\\beta$ matrix. We can do this by pre-multiplying both sides by \"X transpose\" $X^{T}$.\n",
    "\n",
    "\\begin{align}\n",
    "X^{T}y =  X^{T}X \\beta\n",
    "\\end{align}\n",
    "\n",
    "Since anything times its transpose will result in a square matrix, if that matrix is then an invertible matrix, then we should be able to multiply both sides by its inverse to remove it from the right hand side. (We'll talk tomorrow about situations that could lead to $X^{T}X$ not being invertible.)\n",
    "\n",
    "\\begin{align}\n",
    "(X^{T}X)^{-1}X^{T}y =  (X^{T}X)^{-1}X^{T}X \\beta\n",
    "\\end{align}\n",
    "\n",
    "Since any matrix multiplied by its inverse results in the identity matrix, and anything multiplied by the identity matrix is itself, we are left with only $\\beta$ on the right hand side:\n",
    "\n",
    "\\begin{align}\n",
    "(X^{T}X)^{-1}X^{T}y = \\hat{\\beta}\n",
    "\\end{align}\n",
    "\n",
    "We will now call it \"beta hat\" $\\hat{\\beta}$ because it now represents our estimated values for $\\beta_0$ and $\\beta_1$\n",
    "\n",
    "### Lets calculate our $\\beta$ coefficients with numpy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is NOT an objective you'll be tested on. It's just a demo.\n",
    "\n",
    "# X is a matrix. Add constant for the intercept.\n",
    "from statsmodels.api import add_constant\n",
    "X = add_constant(df[feature].values)\n",
    "print('X')\n",
    "print(X)\n",
    "\n",
    "# y is a column vector\n",
    "y = df[target].values[:, np.newaxis]\n",
    "print('y')\n",
    "print(y)\n",
    "\n",
    "# Least squares solution in code\n",
    "X_transpose = X.T\n",
    "X_transpose_X = X_transpose @ X\n",
    "X_transpose_X_inverse = np.linalg.inv(X_transpose_X)\n",
    "X_transpose_y = X_transpose @ y\n",
    "beta_hat = X_transpose_X_inverse @ X_transpose_y\n",
    "\n",
    "print('Beta Hat')\n",
    "print(beta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read New York City property sales data, from first 4 months of 2019.\n",
    "# Dataset has 23040 rows, 21 columns.\n",
    "df = pd.read_csv('../data/NYC_Citywide_Rolling_Calendar_Sales.csv')\n",
    "assert df.shape == (23040, 21)\n",
    "\n",
    "# Change column names. Replace spaces with underscores\n",
    "df.columns = [col.replace(' ', '_') for col in df]\n",
    "\n",
    "# Remove symbols from SALE_PRICE string, convert to integer\n",
    "df['SALE_PRICE'] = (\n",
    "    df['SALE_PRICE']\n",
    "    .str.replace('$','')\n",
    "    .str.replace('-','')\n",
    "    .str.replace(',','')\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Keep subset of rows:\n",
    "# Tribeca neighborhood, Condos - Elevator Apartments, \n",
    "# 1 unit, sale price more than $1, less than $35 million\n",
    "mask = (\n",
    "    (df['NEIGHBORHOOD'].str.contains('TRIBECA')) & \n",
    "    (df['BUILDING_CLASS_CATEGORY'] == '13 CONDOS - ELEVATOR APARTMENTS') &\n",
    "    (df['TOTAL_UNITS'] == 1) & \n",
    "    (df['SALE_PRICE'] > 0) & \n",
    "    (df['SALE_PRICE'] < 35000000)\n",
    ")\n",
    "df = df[mask]\n",
    "\n",
    "# Data now has 90 rows, 21 columns\n",
    "assert df.shape == (90, 21)\n",
    "\n",
    "# Convert SALE_DATE to datetime\n",
    "df['SALE_DATE'] = pd.to_datetime(df['SALE_DATE'], infer_datetime_format=True)\n",
    "\n",
    "from ipywidgets import interact\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Read New York City property sales data, from first 4 months of 2019.\n",
    "# Dataset has 23040 rows, 21 columns.\n",
    "df = pd.read_csv('../data/NYC_Citywide_Rolling_Calendar_Sales.csv')\n",
    "assert df.shape == (23040, 21)\n",
    "\n",
    "# Change column names. Replace spaces with underscores\n",
    "df.columns = [col.replace(' ', '_') for col in df]\n",
    "\n",
    "# Remove symbols from SALE_PRICE string, convert to integer\n",
    "df['SALE_PRICE'] = (\n",
    "    df['SALE_PRICE']\n",
    "    .str.replace('$','')\n",
    "    .str.replace('-','')\n",
    "    .str.replace(',','')\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Keep subset of rows:\n",
    "# Tribeca neighborhood, Condos - Elevator Apartments, \n",
    "# 1 unit, sale price more than $1, less than $35 million\n",
    "mask = (\n",
    "    (df['NEIGHBORHOOD'].str.contains('TRIBECA')) & \n",
    "    (df['BUILDING_CLASS_CATEGORY'] == '13 CONDOS - ELEVATOR APARTMENTS') &\n",
    "    (df['TOTAL_UNITS'] == 1) & \n",
    "    (df['SALE_PRICE'] > 0) & \n",
    "    (df['SALE_PRICE'] < 35000000)\n",
    ")\n",
    "df = df[mask]\n",
    "\n",
    "# Data now has 90 rows, 21 columns\n",
    "assert df.shape == (90, 21)\n",
    "\n",
    "# Convert SALE_DATE to datetime\n",
    "df['SALE_DATE'] = pd.to_datetime(df['SALE_DATE'], infer_datetime_format=True)\n",
    "\n",
    "# Arrange X features matrix & y target vector\n",
    "features = ['GROSS_SQUARE_FEET']\n",
    "target = 'SALE_PRICE'\n",
    "X = df[features]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=11)\n",
    "\n",
    "# Repeatedly fit increasingly complex models, and keep track of the scores\n",
    "polynomial_degrees = range(1, 10, 2)\n",
    "train_r2s = []\n",
    "test_r2s = []\n",
    "\n",
    "for degree in polynomial_degrees:\n",
    "    model = PolynomialRegression(degree)\n",
    "    display(HTML(f'Polynomial degree={degree}'))\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    train_r2 = model.score(X_train, y_train)\n",
    "    test_r2 = model.score(X_test, y_test)\n",
    "    display(HTML(f'<b style=\"color: blue\">Train R2 {train_r2:.2f}</b>'))\n",
    "    display(HTML(f'<b style=\"color: red\">Test R2 {test_r2:.2f}</b>'))\n",
    "\n",
    "    plt.scatter(X_train, y_train, color='blue', alpha=0.5)\n",
    "    plt.scatter(X_test, y_test, color='red', alpha=0.5)\n",
    "    plt.xlabel(features)\n",
    "    plt.ylabel(target)\n",
    "    \n",
    "    x_domain = np.linspace(X.min(), X.max())\n",
    "    curve = model.predict(x_domain)\n",
    "    plt.plot(x_domain, curve, color='blue')\n",
    "    plt.show()\n",
    "    display(HTML('<hr/>'))\n",
    "    \n",
    "    train_r2s.append(train_r2)\n",
    "    test_r2s.append(test_r2)\n",
    "    \n",
    "display(HTML('Validation Curve'))\n",
    "plt.plot(polynomial_degrees, train_r2s, color='blue', label='Train')\n",
    "plt.plot(polynomial_degrees, test_r2s, color='red', label='Test')\n",
    "plt.xlabel('Model Complexity (Polynomial Degree)')\n",
    "plt.ylabel('R^2 Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jake VanderPlas, [_Python Data Science Handbook,_ Chapter 5.3](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#The-Bias-variance-trade-off)\n",
    "\n",
    "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.03-validation-curve.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
